{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Produto escalar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dot product is: 2.1672\n"
     ]
    }
   ],
   "source": [
    "input_vector = [1.72, 1.23]\n",
    "weights_1 = [1.26, 0]\n",
    "weights_2 = [2.17, 0.32]\n",
    "\n",
    "# Computing the dot product of input_vector and weights_1\n",
    "first_indexes_mult = input_vector[0] * weights_1[0]\n",
    "second_indexes_mult = input_vector[1] * weights_1[1]\n",
    "dot_product_1 = first_indexes_mult + second_indexes_mult\n",
    "\n",
    "print(f\"The dot product is: {dot_product_1}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "produto escalar com numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dot product is: 2.1672\n",
      "The dot product is: 4.1259999999999994\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "dot_product_1 = np.dot(input_vector, weights_1)\n",
    "\n",
    "print(f\"The dot product is: {dot_product_1}\")\n",
    "\n",
    "dot_product_2 = np.dot(input_vector, weights_2)\n",
    "\n",
    "print(f\"The dot product is: {dot_product_2}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "definição função sigmoid e neuronio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The prediction result is: [0.7985731]\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Wrapping the vectors in NumPy arrays\n",
    "input_vector = np.array([1.66, 1.56])\n",
    "weights_1 = np.array([1.45, -0.66])\n",
    "bias = np.array([0.0])\n",
    "\n",
    "'''def sigmoid(x):\n",
    "    return 1 / (1 + 2.7182 ** -x)'''\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def make_prediction(input_vector, weights, bias):\n",
    "     layer_1 = np.dot(input_vector, weights) + bias\n",
    "     layer_2 = sigmoid(layer_1)\n",
    "     return layer_2\n",
    "\n",
    "prediction = make_prediction(input_vector, weights_1, bias)\n",
    "\n",
    "print(f\"The prediction result is: {prediction}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculando o erro de previsão\n",
    "Para entender a magnitude do erro, você precisa escolher uma forma de medi-lo. A função usada para medir o erro é chamada de função de custo ou função de perda . Neste tutorial, você usará o erro quadrático médio (MSE) como função de custo. Você calcula o MSE em duas etapas:\n",
    "\n",
    "* Calcule a diferença entre a previsão e o alvo.\n",
    "* Multiplique o resultado por si mesmo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The prediction result is: [0.87101915]\n",
      "Prediction: [0.87101915]; Error: [0.75867436]\n"
     ]
    }
   ],
   "source": [
    "input_vector = np.array([2, 1.5])\n",
    "\n",
    "prediction = make_prediction(input_vector, weights_1, bias)\n",
    "\n",
    "print(f\"The prediction result is: {prediction}\")\n",
    "target = 0\n",
    "\n",
    "mse = (prediction - target) ** 2\n",
    "\n",
    "print(f\"Prediction: {prediction}; Error: {mse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para saber qual direção você deve seguir para reduzir o erro, você usará a derivada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The derivative is [1.7420383]\n"
     ]
    }
   ],
   "source": [
    "derivative = 2 * (prediction - target)\n",
    "\n",
    "print(f\"The derivative is {derivative}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "atualize os pesos  O resultado é 1.74um número positivo, então você precisa diminuir os pesos. se fosse negatimo vc aumentaria os pesos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: [0.01496248]; Error: [0.00022388]\n"
     ]
    }
   ],
   "source": [
    "# Updating the weights\n",
    "weights_1 = weights_1 - derivative\n",
    "\n",
    "prediction = make_prediction(input_vector, weights_1, bias)\n",
    "\n",
    "error = (prediction - target) ** 2\n",
    "\n",
    "print(f\"Prediction: {prediction}; Error: {error}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " resultado da derivada foi pequeno, mas há alguns casos em que o resultado da derivada é muito alto. Tomemos como exemplo a imagem da função quadrática. Incrementos altos não são ideais porque você pode continuar indo de um ponto Aa outro B, nunca chegando perto de zero. Para lidar com isso, você atualiza os pesos com uma fração do resultado da derivada. \n",
    "\n",
    " * Para definir uma fração para atualização dos pesos, utiliza-se o parâmetro alfa , também chamado de taxa de aprendizagem ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ajustando os parâmetros com retropropagação"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A derivada da função sigmoidal em um ponto específico nos diz a inclinação da curva sigmoidal naquele ponto. Se a inclinação for alta, isso significa que pequenas mudanças nos pesos e vieses terão um grande impacto na saída da rede. Se a inclinação for baixa, as mesmas mudanças terão um impacto menor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_deriv(x):\n",
    "    return sigmoid(x) * (1-sigmoid(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Em seguida, apos calcularmos o erro,  calculamos a derivada da previsão em relação à camada anterior, dprediction_dlayer1. Isso envolve aplicar a função de ativação sigmoidal (ou sua derivada) à saída da primeira camada oculta layer_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "derror_dprediction = 2 * (prediction - target)\n",
    "layer_1 = np.dot(input_vector, weights_1) + bias\n",
    "dprediction_dlayer1 = sigmoid_deriv(layer_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depois, calculamos a derivada da camada anterior em relação ao viés, dlayer1_dbias. Para o viés, essa derivada é simplesmente 1, pois o viés é uma constante e não afeta a operação de multiplicação.\n",
    "\n",
    "Por fim, calculamos a derivada do erro em relação ao viés, derror_dbias, multiplicando todas as derivadas parciais calculadas anteriormente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.00044105]\n"
     ]
    }
   ],
   "source": [
    "dlayer1_dbias = 1\n",
    "derror_dbias = derror_dprediction * dprediction_dlayer1 * dlayer1_dbias\n",
    "\n",
    "print(derror_dbias)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_vectors:\n",
      "[[1.7816321  0.51326134]\n",
      " [1.6955208  0.51879784]\n",
      " [1.79610581 0.63938874]\n",
      " [1.51508091 0.58168608]\n",
      " [1.60260462 0.50106354]\n",
      " [1.71014512 0.82845613]\n",
      " [1.56900905 0.73324729]\n",
      " [1.98582404 0.51125307]\n",
      " [1.59520201 0.81162418]\n",
      " [1.72285972 0.79511961]\n",
      " [1.92277029 0.67548094]\n",
      " [1.89126047 0.74399902]\n",
      " [1.57844405 0.55246718]\n",
      " [1.67830665 0.67208326]\n",
      " [1.55919202 0.69625579]\n",
      " [1.78071716 0.47649618]\n",
      " [1.73365765 0.54915754]\n",
      " [1.6922108  0.64293403]\n",
      " [1.71265598 0.78873404]\n",
      " [1.82231211 0.54295758]\n",
      " [1.83855417 0.54199252]\n",
      " [1.65732669 0.58485115]\n",
      " [1.82720558 0.70403281]\n",
      " [1.67663792 0.5664051 ]\n",
      " [1.75267411 0.5614793 ]\n",
      " [1.51930129 0.56252067]\n",
      " [1.58640822 0.61040551]\n",
      " [1.77176242 0.8881861 ]\n",
      " [1.55210126 0.79948541]\n",
      " [1.8093738  0.48444561]\n",
      " [1.57740351 0.73686573]\n",
      " [1.68425496 0.47323762]\n",
      " [1.56194889 0.49155716]\n",
      " [1.88692066 0.56739879]\n",
      " [1.63124464 0.73739093]\n",
      " [1.56675894 0.49201021]\n",
      " [1.53047886 0.45398924]\n",
      " [1.50895289 0.72113362]\n",
      " [1.63517247 0.8705967 ]\n",
      " [1.77815492 0.50684277]\n",
      " [1.86945702 0.79519334]\n",
      " [1.99623604 0.75370339]\n",
      " [1.6127464  0.66696035]\n",
      " [1.8677831  0.773547  ]\n",
      " [1.8341972  0.5771285 ]\n",
      " [1.89044083 0.48502002]\n",
      " [1.79396435 0.6546376 ]\n",
      " [1.97070999 0.67950597]\n",
      " [1.78533665 0.66406519]\n",
      " [1.6196876  0.74289293]]\n",
      "\n",
      "targets:\n",
      "[0.1616972  0.18046456 0.19819886 0.25340607 0.19509225 0.28327196\n",
      " 0.29785135 0.12964459 0.31895073 0.26787527 0.18270831 0.20800304\n",
      " 0.22174207 0.2386055  0.28639776 0.15026905 0.18271335 0.22452127\n",
      " 0.2688997  0.16350099 0.16033947 0.2129262  0.21087159 0.20148772\n",
      " 0.18278115 0.24369727 0.24254289 0.28293912 0.33187187 0.14797503\n",
      " 0.29614388 0.16682594 0.20148392 0.15936066 0.27711472 0.20043324\n",
      " 0.19381656 0.3167119  0.32560411 0.1603002  0.22753162 0.18913708\n",
      " 0.25642942 0.22173478 0.17154595 0.13571676 0.20341048 0.17496365\n",
      " 0.20833895 0.28318074]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def calcular_imc(altura, peso):\n",
    "    return  peso / (altura ** 2)\n",
    "    \n",
    "\n",
    "def gerar_dados(num_samples):\n",
    "    alturas = np.random.uniform(1.5, 2, num_samples)\n",
    "    pesos = np.random.uniform(0.45, 0.90, num_samples)\n",
    "    imcs = [calcular_imc(alturas[i], pesos[i]) for i in range(num_samples)]\n",
    "    input_vectors = np.column_stack((alturas, pesos))\n",
    "    targets = np.array(imcs)\n",
    "    return input_vectors, targets\n",
    "\n",
    "input_vectors, targets = gerar_dados(50)\n",
    "\n",
    "print(\"input_vectors:\")\n",
    "print(input_vectors)\n",
    "print(\"\\ntargets:\")\n",
    "print(targets)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
